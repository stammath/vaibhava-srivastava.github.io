<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>

<head>
<title>twitmining.bib</title>
</head>

<body>
<h1>twitmining.bib</h1><a name="Guo20122008"></a><pre>
@article{<a href="twitmining.html#Guo20122008">Guo20122008</a>,
  title = {Mining Hot Topics from Twitter Streams },
  journal = {Procedia Computer Science },
  volume = {9},
  number = {},
  pages = {2008 - 2011},
  year = {2012},
  note = {Proceedings of the International Conference on Computational Science, \{ICCS\} 2012 },
  issn = {1877-0509},
  doi = {<a href="http://dx.doi.org/10.1016/j.procs.2012.04.224">http://dx.doi.org/10.1016/j.procs.2012.04.224</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S1877050912003456">http://www.sciencedirect.com/science/article/pii/S1877050912003456</a>},
  author = {Jing Guo and Peng Zhang and JianlongTan and Li Guo},
  keywords = {Data stream mining},
  keywords = {Hot topic mining},
  keywords = {Frequent pattern mining},
  keywords = {Twitter streams },
  abstract = {Mininghottopicsfrom twitter streamshas attractedalotof attentionin recent years.Traditionalhottopicmining from InternetWeb pages were mainly basedontext clustering.However, comparedtothetextsinWeb pages, twitter texts are relatively short with sparse attributes. Moreover,twitter data often increase rapidly withfast spreading speed, whichposesgreat challengetoexistingtopicmining models.Tothisend,we propose,inthispaper, aﬂexible stream mining approach for hot twitter topic detection. Speciﬁcally, we propose to use the FrequentPattern stream mining algorithm (i.e. FP-stream) to detect hot topics from twitter streams. Empirical studies on real world twitter data demonstrate the utility of the proposed method. }
}
</pre>

<a name="Park2015208"></a><pre>
@article{<a href="twitmining.html#Park2015208">Park2015208</a>,
  title = {Comparing Twitter and YouTube networks in information diffusion: The case of the “Occupy Wall Street” movement },
  journal = {Technological Forecasting and Social Change },
  volume = {95},
  number = {},
  pages = {208 - 217},
  year = {2015},
  note = {},
  issn = {0040-1625},
  doi = {<a href="http://dx.doi.org/10.1016/j.techfore.2015.02.003">http://dx.doi.org/10.1016/j.techfore.2015.02.003</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0040162515000190">http://www.sciencedirect.com/science/article/pii/S0040162515000190</a>},
  author = {Se Jung Park and Yon Soo Lim and Han Woo Park},
  keywords = {Information diffusion},
  keywords = {Twitter network},
  keywords = {YouTube network},
  keywords = {Occupy Wall Street movement},
  keywords = {Protests on social media},
  keywords = {Network theory},
  keywords = {Social network analysis },
  abstract = {Abstract Grounded by the micro approach to network theory, information diffusion theory, and the web ecology model, this study comparatively explores the network structure, interaction pattern, and geographic distribution of users involved in communication networks of the Occupy Wall Street movement on Twitter and YouTube. The results show that Twitter users generated a loosely connected hub-and-spoke network, suggesting that information was likely to be organized by several central users in the network and that these users bridged small communities. On YouTube, homogeneously themed videos formed a dense mesh network, reinforcing shared ideas and meanings. According to the geographic distribution, both Twitter and YouTube networks were actively organized by U.S. users, but the YouTube network was activated mainly by anonymous users. These results highlight differing roles of social media in political information diffusion in which the Twitter network not only organizes and coordinates information but also facilitate the exchange of ideas between different groups. YouTube is suitable for disseminating ideas and reinforcing solidarity among members. The results demonstrate useful analytical techniques for data mining and analyzing Twitter and YouTube networks and have important implications for distinct roles of social media platforms in organizing collective action. }
}
</pre>

<a name="Cha201597"></a><pre>
@article{<a href="twitmining.html#Cha201597">Cha201597</a>,
  title = {Mining web-based data to assess public response to environmental events },
  journal = {Environmental Pollution },
  volume = {198},
  number = {},
  pages = {97 - 99},
  year = {2015},
  note = {},
  issn = {0269-7491},
  doi = {<a href="http://dx.doi.org/10.1016/j.envpol.2014.12.027">http://dx.doi.org/10.1016/j.envpol.2014.12.027</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0269749114005272">http://www.sciencedirect.com/science/article/pii/S0269749114005272</a>},
  author = {YoonKyung Cha and Craig A. Stow},
  keywords = {Twitter},
  keywords = {Google trends},
  keywords = {Social media},
  keywords = {Web search trends},
  keywords = {Data mining},
  keywords = {Algal blooms},
  keywords = {Public perception and interest },
  abstract = {Abstract We explore how the analysis of web-based data, such as Twitter and Google Trends, can be used to assess the social relevance of an environmental accident. The concept and methods are applied in the shutdown of drinking water supply at the city of Toledo, Ohio, USA. Toledo's notice, which persisted from August 1 to 4, 2014, is a high-profile event that directly influenced approximately half a million people and received wide recognition. The notice was given when excessive levels of microcystin, a byproduct of cyanobacteria blooms, were discovered at the drinking water treatment plant on Lake Erie. Twitter mining results illustrated an instant response to the Toledo incident, the associated collective knowledge, and public perception. The results from Google Trends, on the other hand, revealed how the Toledo event raised public attention on the associated environmental issue, harmful algal blooms, in a long-term context. Thus, when jointly applied, Twitter and Google Trend analysis results offer complementary perspectives. Web content aggregated through mining approaches provides a social standpoint, such as public perception and interest, and offers context for establishing and evaluating environmental management policies. }
}
</pre>

<a name="Burnap201596"></a><pre>
@article{<a href="twitmining.html#Burnap201596">Burnap201596</a>,
  title = {Detecting tension in online communities with computational Twitter analysis },
  journal = {Technological Forecasting and Social Change },
  volume = {95},
  number = {},
  pages = {96 - 108},
  year = {2015},
  note = {},
  issn = {0040-1625},
  doi = {<a href="http://dx.doi.org/10.1016/j.techfore.2013.04.013">http://dx.doi.org/10.1016/j.techfore.2013.04.013</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0040162513000899">http://www.sciencedirect.com/science/article/pii/S0040162513000899</a>},
  author = {Pete Burnap and Omer F. Rana and Nick Avis and Matthew Williams and William Housley and Adam Edwards and Jeffrey Morgan and Luke Sloan},
  keywords = {Opinion mining},
  keywords = {Sentiment analysis},
  keywords = {Text mining},
  keywords = {Social media analysis},
  keywords = {Machine learning},
  keywords = {Conversation analysis},
  keywords = {Membership categorization analysis },
  abstract = {Abstract The growing number of people using social media to communicate with others and document their personal opinion and action is creating a significant stream of data that provides the opportunity for social scientists to conduct online forms of research, providing an insight into online social formations. This paper investigates the possibility of forecasting spikes in social tension – defined by the \{UK\} police service as “any incident that would tend to show that the normal relationship between individuals or groups has seriously deteriorated” – through social media. A number of different computational methods were trialed to detect spikes in tension using a human coded sample of data collected from Twitter, relating to an accusation of racial abuse during a Premier League football match. Conversation analysis combined with syntactic and lexicon-based text mining rules; sentiment analysis; and machine learning methods was tested as a possible approach. Results indicate that a combination of conversation analysis methods and text mining outperforms a number of machine learning approaches and a sentiment analysis tool at classifying tension levels in individual tweets. }
}
</pre>

<a name="Shen2015"></a><pre>
@article{<a href="twitmining.html#Shen2015">Shen2015</a>,
  title = {Learning in massive open online courses: Evidence from social media mining },
  journal = {Computers in Human Behavior },
  volume = {},
  number = {},
  pages = { - },
  year = {2015},
  note = {},
  issn = {0747-5632},
  doi = {<a href="http://dx.doi.org/10.1016/j.chb.2015.02.066">http://dx.doi.org/10.1016/j.chb.2015.02.066</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0747563215002083">http://www.sciencedirect.com/science/article/pii/S0747563215002083</a>},
  author = {Chien-wen Shen and Chin-Jin Kuo},
  keywords = {MOOC},
  keywords = {Learning},
  keywords = {Social media},
  keywords = {Data mining},
  keywords = {Sentiment analysis},
  keywords = {Social network },
  abstract = {Abstract Because many massive open online courses (MOOCs) have adopted social media tools for large student audiences to co-create knowledge and engage in collective learning processes, this study adopted various social media mining approaches to investigate Twitter messages related to \{MOOC\} learning. The first approach adopted in this study was calculating the important descriptive statistics of MOOC-related tweets and examining the daily, weekly, and monthly trends of \{MOOC\} that appeared on Twitter. This information can enable \{MOOC\} practitioners to observe participants’ temporal activities on social media and ascertain the most effective time to post or analyze tweets. Secondly, we investigated how public sentiment toward \{MOOC\} learning can be assessed according to related tweets. Because the availability and popularity of opinion-rich social networking services are increasing for \{MOOC\} communities, our findings from the sentiment analysis of Twitter data can afford substantial insights into participant perceptions of \{MOOC\} learning. Third, we analyzed the positive and negative retweets related to \{MOOCs\} and identified the influencers of these retweets. Social network diagrams were also developed to reveal how sentimental messages about \{MOOCs\} on Twitter were disseminated from the top influencers with the highest number of positive/negative retweets about MOOCs. Analyzing the relationships among top retweet users is vital to \{MOOC\} practitioners because they can use this information to filter or recommend MOOC-related messages to the influencers. In short, the findings pertaining social media mining in this study afford a holistic understanding of \{MOOC\} trends, public sentiment toward \{MOOC\} learning, and the influencers of MOOC-related retweets. }
}
</pre>

<a name="Odlum2015563"></a><pre>
@article{<a href="twitmining.html#Odlum2015563">Odlum2015563</a>,
  title = {What can we learn about the Ebola outbreak from tweets? },
  journal = {American Journal of Infection Control },
  volume = {43},
  number = {6},
  pages = {563 - 571},
  year = {2015},
  note = {},
  issn = {0196-6553},
  doi = {<a href="http://dx.doi.org/10.1016/j.ajic.2015.02.023">http://dx.doi.org/10.1016/j.ajic.2015.02.023</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0196655315001376">http://www.sciencedirect.com/science/article/pii/S0196655315001376</a>},
  author = {Michelle Odlum and Sunmoo Yoon},
  keywords = {Ebola outbreak},
  keywords = {Social media},
  keywords = {Data mining },
  abstract = {Background Twitter can address the challenges of the current Ebola outbreak surveillance. The aims of this study are to demonstrate the use of Twitter as a real-time method of Ebola outbreak surveillance to monitor information spread, capture early epidemic detection, and examine content of public knowledge and attitudes. Methods We collected tweets mentioning Ebola in English during the early stage of the current Ebola outbreak from July 24-August 1, 2014. Our analysis for this observational study includes time series analysis with geologic visualization to observe information dissemination and content analysis using natural language processing to examine public knowledge and attitudes. Results A total of 42,236 tweets (16,499 unique and 25,737 retweets) mentioning Ebola were posted and disseminated to 9,362,267,048 people, 63 times higher than the initial number. Tweets started to rise in Nigeria 3-7 days prior to the official announcement of the first probable Ebola case. The topics discussed in tweets include risk factors, prevention education, disease trends, and compassion. Conclusion Because of the analysis of a unique Twitter dataset captured in the early stage of the current Ebola outbreak, our results provide insight into the intersection of social media and public health outbreak surveillance. Findings demonstrate the usefulness of Twitter mining to inform public health education. }
}
</pre>

<a name="Saleem2014165"></a><pre>
@article{<a href="twitmining.html#Saleem2014165">Saleem2014165</a>,
  title = {Effects of Disaster Characteristics on Twitter Event Signature },
  journal = {Procedia Engineering },
  volume = {78},
  number = {},
  pages = {165 - 172},
  year = {2014},
  note = {Humanitarian Technology: Science, Systems and Global Impact 2014, HumTech2014 },
  issn = {1877-7058},
  doi = {<a href="http://dx.doi.org/10.1016/j.proeng.2014.07.053">http://dx.doi.org/10.1016/j.proeng.2014.07.053</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S1877705814010406">http://www.sciencedirect.com/science/article/pii/S1877705814010406</a>},
  author = {Haji Mohammad Saleem and Yishi Xu and Derek Ruths},
  keywords = {Twitter event analysis},
  keywords = {Crisis Informatics},
  keywords = {Humanitarian Data Mining},
  keywords = {Social media},
  keywords = {Hashtags. },
  abstract = {Abstract Twitter has emerged as a platform that is heavily used during disasters. Therefore, as an event unfolds, it generates varying levels of online engagement from victims as well as onlookers (both physical and virtual). Because methods for mining disaster-related content at scale must contend with the problem of filtering out vast numbers of unrelated posts, any prior knowledge about the characteristics of disaster-related content in the live Twitter feed may help improve the recovery of relevant posts. In this study, we consider the relative abundance of a disasters Twitter content over time (both relative to total event-related content and relative to the overall volume of content generated on Twitter). We refer to this time-varying abundance as the events signature. In an analysis of three different disasters, we find that event signatures are qualitatively different. These differences can be explained in terms of several characteristics of disasters: foreknowledge, duration, severity, and news media engagement. }
}
</pre>

<a name="Chae2015247"></a><pre>
@article{<a href="twitmining.html#Chae2015247">Chae2015247</a>,
  title = {Insights from hashtag #supplychain and Twitter Analytics: Considering Twitter and Twitter data for supply chain practice and research },
  journal = {International Journal of Production Economics },
  volume = {165},
  number = {},
  pages = {247 - 259},
  year = {2015},
  note = {},
  issn = {0925-5273},
  doi = {<a href="http://dx.doi.org/10.1016/j.ijpe.2014.12.037">http://dx.doi.org/10.1016/j.ijpe.2014.12.037</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0925527314004319">http://www.sciencedirect.com/science/article/pii/S0925527314004319</a>},
  author = {Bongsug (Kevin) Chae},
  keywords = {Supply chain management},
  keywords = {Twitter},
  keywords = {Data analytics},
  keywords = {Network analytics},
  keywords = {Content analytics},
  keywords = {Big data},
  keywords = {Social media analytics},
  keywords = {Application Programming Interface (API) },
  abstract = {Abstract Recently, businesses and research communities have paid a lot of attention to social media and big data. However, the field of supply chain management (SCM) has been relatively slow in studying social media and big data for research and practice. In these contexts, this research contributes to the \{SCM\} community by proposing a novel, analytical framework (Twitter Analytics) for analyzing supply chain tweets, highlighting the current use of Twitter in supply chain contexts, and further developing insights into the potential role of Twitter for supply chain practice and research. The proposed framework combines three methodologies – descriptive analytics (DA), content analytics (CA) integrating text mining and sentiment analysis, and network analytics (NA) relying on network visualization and metrics – for extracting intelligence from 22,399 #supplychain tweets. Some of the findings are: supply chain tweets are used by different groups of supply chain professionals and organizations (e.g., news services, \{IT\} companies, logistic providers, manufacturers) for information sharing, hiring professionals, and communicating with stakeholders, among others; diverse topics are being discussed, ranging from logistics and corporate social responsibility, to risk, manufacturing, \{SCM\} \{IT\} and even human rights; some tweets carry strong sentiments about companies׳ delivery services, sales performance, and environmental standards, and risk and disruption in supply chains. Based on these findings, this research presents insights into the use and potential role of Twitter for supply chain practices (e.g., professional networking, stakeholder engagement, demand shaping, new product/service development, supply chain risk management) and the implications for research. Finally, the limitations of the current study and suggestions for future research are presented. }
}
</pre>

<a name="Kirilenko2014171"></a><pre>
@article{<a href="twitmining.html#Kirilenko2014171">Kirilenko2014171</a>,
  title = {Public microblogging on climate change: One year of Twitter worldwide },
  journal = {Global Environmental Change },
  volume = {26},
  number = {},
  pages = {171 - 182},
  year = {2014},
  note = {},
  issn = {0959-3780},
  doi = {<a href="http://dx.doi.org/10.1016/j.gloenvcha.2014.02.008">http://dx.doi.org/10.1016/j.gloenvcha.2014.02.008</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0959378014000375">http://www.sciencedirect.com/science/article/pii/S0959378014000375</a>},
  author = {Andrei P. Kirilenko and Svetlana O. Stepchenkova},
  keywords = {Climate change},
  keywords = {Communication},
  keywords = {Blogging},
  keywords = {Twitter},
  keywords = {Content analysis },
  abstract = {Abstract Public perceptions of climate change are traditionally measured through surveys. The exploding popularity of social networks, however, presents a new opportunity to research the spatiotemporal pattern of public discourse in relation to natural and/or socio-economic events. Among the social networks, Twitter is one of the largest microblogging services. The architecture of Twitter makes the question “what's happening?” the cornerstone of information exchange. This inspired the notion of using Twitter users as distributed sensors, which has been successfully employed in both the natural and social sciences. In 2012 and 2013, we collected 1.8 million tweets on “climate change” and “global warming” in five major languages (English, German, Russian, Portuguese, and Spanish). We discuss the geography of tweeting, weekly and daily patterns, major news events that affected tweeting on climate change, changes in the central topics of discussion over time, the most authoritative traditional media, blogging, and the most authoritative organizational sources of information on climate change referenced by Twitter users in different countries. We anticipate that social network mining will become a major source of data in the public discourse on climate change. }
}
</pre>

<a name="Yang2015217"></a><pre>
@article{<a href="twitmining.html#Yang2015217">Yang2015217</a>,
  title = {\{GIS\} analysis of depression among Twitter users },
  journal = {Applied Geography },
  volume = {60},
  number = {},
  pages = {217 - 223},
  year = {2015},
  note = {},
  issn = {0143-6228},
  doi = {<a href="http://dx.doi.org/10.1016/j.apgeog.2014.10.016">http://dx.doi.org/10.1016/j.apgeog.2014.10.016</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0143622814002537">http://www.sciencedirect.com/science/article/pii/S0143622814002537</a>},
  author = {Wei Yang and Lan Mu},
  keywords = {Depression},
  keywords = {Tweets},
  keywords = {Clustering},
  keywords = {GIS},
  keywords = {Social media },
  abstract = {Abstract Depression is a common chronic disorder. It often goes undetected due to limited diagnosis methods and brings serious results to public and personal health. Former research detected geographic pattern for depression using questionnaires or self-reported measures of mental health, this may induce same-source bias. Recent studies use social media for depression detection but none of them examines the geographic patterns. In this paper, we apply \{GIS\} methods to social media data to provide new perspectives for public health research. We design a procedure to automatically detect depressed users in Twitter and analyze their spatial patterns using \{GIS\} technology. This method can improve diagnosis techniques for depression. It is faster at collecting data and more promptly at analyzing and providing results. Also, this method can be expanded to detect other major events in real-time, such as disease outbreaks and earthquakes. }
}
</pre>

<a name="Yang2015184"></a><pre>
@article{<a href="twitmining.html#Yang2015184">Yang2015184</a>,
  title = {Effect of climate and seasonality on depressed mood among twitter users },
  journal = {Applied Geography },
  volume = {63},
  number = {},
  pages = {184 - 191},
  year = {2015},
  note = {},
  issn = {0143-6228},
  doi = {<a href="http://dx.doi.org/10.1016/j.apgeog.2015.06.017">http://dx.doi.org/10.1016/j.apgeog.2015.06.017</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0143622815001617">http://www.sciencedirect.com/science/article/pii/S0143622815001617</a>},
  author = {Wei Yang and Lan Mu and Ye Shen},
  keywords = {Climate},
  keywords = {Depression},
  keywords = {GIS},
  keywords = {Seasonality},
  keywords = {Social media},
  keywords = {Twitter },
  abstract = {Abstract Location-based social media provide an enormous stream of data about humans' life and behavior. With geospatial methods, those data can offer rich insights into public health. In this research, we study the effect of climate and seasonality on the prevalence of depression in Twitter users in the U.S. Text mining and geospatial methods are used to detect tweets related to depression and their spatiotemporal patterns at the scale of Metropolitan Statistical Area. We find the relationship between depression rates, climate risk factors and seasonality are varied and geographically localized. The same climate measure may have opposite association with depression rates at different places. Relative humidity, temperature, sea level pressure, precipitation, snowfall, weed speed, globe solar radiation, and length of day all contribute to the geographic variations of depression rates. A conceptual compact map is designed to visualize scattered geographic phenomena in a large area. We also propose a three-stage framework that semi-automatically detects and analyzes geographically distributed health issues using location-based social media data. }
}
</pre>

<a name="CampoÁvila2013437"></a><pre>
@article{<a href="twitmining.html#CampoÁvila2013437">CampoÁvila2013437</a>,
  title = {Bridging the Gap Between the Least and the Most Influential Twitter Users },
  journal = {Procedia Computer Science },
  volume = {19},
  number = {},
  pages = {437 - 444},
  year = {2013},
  note = {The 4th International Conference on Ambient Systems, Networks and Technologies (ANT 2013), the 3rd International Conference on Sustainable Energy Information Technology (SEIT-2013) },
  issn = {1877-0509},
  doi = {<a href="http://dx.doi.org/10.1016/j.procs.2013.06.059">http://dx.doi.org/10.1016/j.procs.2013.06.059</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S1877050913006674">http://www.sciencedirect.com/science/article/pii/S1877050913006674</a>},
  author = {J. del Campo-Ávila and N. Moreno-Vergara and M. Trella-López},
  keywords = {Social-Media networking},
  keywords = {Twitter influence},
  keywords = {Data Mining },
  abstract = {Abstract Social networks play an increasingly important role in shaping the behaviour of users of the Web. Conceivably Twitter stands out from the others, not only for the platform's simplicity but also for the great influence that the messages sent over the network can have. The impact of such messages determines the influence of a Twitter user and is what tools such as Klout, PeerIndex or TwitterGrader aim to calculate. Reducing all the factors that make a person influential into a single number is not an easy task, and the effort involved could become useless if the Twitter users do not know how to improve it. In this paper we identify what specific actions should be carried out for a Twitterer to increase their influence in each of above-mentioned tools applying, for this purpose, data mining techniques based on classification and regression algorithms to the information collected from a set of Twitter users. }
}
</pre>

<a name="Cagliero201416"></a><pre>
@article{<a href="twitmining.html#Cagliero201416">Cagliero201416</a>,
  title = {Twitter data analysis by means of Strong Flipping Generalized Itemsets },
  journal = {Journal of Systems and Software },
  volume = {94},
  number = {},
  pages = {16 - 29},
  year = {2014},
  note = {},
  issn = {0164-1212},
  doi = {<a href="http://dx.doi.org/10.1016/j.jss.2014.03.060">http://dx.doi.org/10.1016/j.jss.2014.03.060</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0164121214000806">http://www.sciencedirect.com/science/article/pii/S0164121214000806</a>},
  author = {Luca Cagliero and Tania Cerquitelli and Paolo Garza and Luigi Grimaudo},
  keywords = {Social network analysis and mining},
  keywords = {Data mining and knowledge discovery},
  keywords = {Generalized itemset mining },
  abstract = {Abstract Twitter data has recently been considered to perform a large variety of advanced analysis. Analysis of Twitter data imposes new challenges because the data distribution is intrinsically sparse, due to a large number of messages post every day by using a wide vocabulary. Aimed at addressing this issue, generalized itemsets – sets of items at different abstraction levels – can be effectively mined and used to discover interesting multiple-level correlations among data supplied with taxonomies. Each generalized itemset is characterized by a correlation type (positive, negative, or null) according to the strength of the correlation among its items. This paper presents a novel data mining approach to supporting different and interesting targeted analysis – topic trend analysis, context-aware service profiling – by analyzing Twitter posts. We aim at discovering contrasting situations by means of generalized itemsets. Specifically, we focus on comparing itemsets discovered at different abstraction levels and we select large subsets of specific (descendant) itemsets that show correlation type changes with respect to their common ancestor. To this aim, a novel kind of pattern, namely the Strong Flipping Generalized Itemset (SFGI), is extracted from Twitter messages and contextual information supplied with taxonomy hierarchies. Each \{SFGI\} consists of a frequent generalized itemset X and the set of its descendants showing a correlation type change with respect to X. Experiments performed on both real and synthetic datasets demonstrate the effectiveness of the proposed approach in discovering interesting and hidden knowledge from Twitter data. }
}
</pre>

<a name="Stevens201515"></a><pre>
@article{<a href="twitmining.html#Stevens201515">Stevens201515</a>,
  title = {Sources of spatial animal and human health data: Casting the net wide to deal more effectively with increasingly complex disease problems },
  journal = {Spatial and Spatio-temporal Epidemiology },
  volume = {13},
  number = {},
  pages = {15 - 29},
  year = {2015},
  note = {},
  issn = {1877-5845},
  doi = {<a href="http://dx.doi.org/10.1016/j.sste.2015.04.003">http://dx.doi.org/10.1016/j.sste.2015.04.003</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S1877584515000179">http://www.sciencedirect.com/science/article/pii/S1877584515000179</a>},
  author = {Kim B. Stevens and Dirk U. Pfeiffer},
  keywords = {Big data},
  keywords = {Data warehouse},
  keywords = {Google Earth},
  keywords = {mHealth},
  keywords = {Spatial data},
  keywords = {Volunteered geographic information },
  abstract = {Abstract During the last 30 years it has become commonplace for epidemiological studies to collect locational attributes of disease data. Although this advancement was driven largely by the introduction of handheld global positioning systems (GPS), and more recently, smartphones and tablets with built-in GPS, the collection of georeferenced disease data has moved beyond the use of handheld \{GPS\} devices and there now exist numerous sources of crowdsourced georeferenced disease data such as that available from georeferencing of Google search queries or Twitter messages. In addition, cartography has moved beyond the realm of professionals to crowdsourced mapping projects that play a crucial role in disease control and surveillance of outbreaks such as the 2014 West Africa Ebola epidemic. This paper provides a comprehensive review of a range of innovative sources of spatial animal and human health data including data warehouses, mHealth, Google Earth, volunteered geographic information and mining of internet-based big data sources such as Google and Twitter. We discuss the advantages, limitations and applications of each, and highlight studies where they have been used effectively. }
}
</pre>

<a name="Kaneko2015"></a><pre>
@article{<a href="twitmining.html#Kaneko2015">Kaneko2015</a>,
  title = {Event photo mining from Twitter using keyword bursts and image clustering },
  journal = {Neurocomputing },
  volume = {},
  number = {},
  pages = { - },
  year = {2015},
  note = {},
  issn = {0925-2312},
  doi = {<a href="http://dx.doi.org/10.1016/j.neucom.2015.02.081">http://dx.doi.org/10.1016/j.neucom.2015.02.081</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0925231215006050">http://www.sciencedirect.com/science/article/pii/S0925231215006050</a>},
  author = {Takamu Kaneko and Keiji Yanai},
  keywords = {Twitter},
  keywords = {Microblog},
  keywords = {Geotagged image},
  keywords = {Event mining},
  keywords = {Event photo mining},
  keywords = {Geo-photo tweet },
  abstract = {Abstract Twitter is a unique microblogging service which enables people to post and read not only short messages but also photos from anywhere. Since microblogs are different from traditional blogs in terms of timeliness and on-the-spot-ness, they include much information on various events over the world. Especially, photos posted to microblogs are useful to understand what happens in the world visually and intuitively. In this paper, we propose a system to discover events and related photos from the Twitter stream. We make use of “geo-photo tweets” which are tweets including both geotags and photos in order to mine various events visually and geographically. Some works on event mining which utilize geotagged tweets have been proposed so far. However, they used no images but only textual analysis of tweet message texts. In this work, we detect events using visual information as well as textual information. In the experiments, we analyzed 17 million geo-photo tweets posted in the United States and 3 million geo-photo tweets posted in Japan with the proposed method, and evaluated the results. We show some examples of detected events and their photos such as “rainbow”, “fireworks” “Tokyo firefly festival” and “Halloween”. }
}
</pre>

<a name="Basari2013453"></a><pre>
@article{<a href="twitmining.html#Basari2013453">Basari2013453</a>,
  title = {Opinion Mining of Movie Review using Hybrid Method of Support Vector Machine and Particle Swarm Optimization },
  journal = {Procedia Engineering },
  volume = {53},
  number = {},
  pages = {453 - 462},
  year = {2013},
  note = {Malaysian Technical Universities Conference on Engineering &amp;amp;amp; Technology 2012, \{MUCET\} 2012 },
  issn = {1877-7058},
  doi = {<a href="http://dx.doi.org/10.1016/j.proeng.2013.02.059">http://dx.doi.org/10.1016/j.proeng.2013.02.059</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S1877705813001781">http://www.sciencedirect.com/science/article/pii/S1877705813001781</a>},
  author = {Abd. Samad Hasan Basari and Burairah Hussin and I. Gede Pramudya Ananta and Junta Zeniarja},
  keywords = {Opinion},
  keywords = {Opinion mining},
  keywords = {Sentiment},
  keywords = {Sentiment analysis},
  keywords = {SVM},
  keywords = {SVM-PSO },
  abstract = {Nowadays, online social media is online discourse where people contribute to create content, share it, bookmark it, and network at an impressive rate. The faster message and ease of use in social media today is Twitter. The messages on Twitter include reviews and opinions on certain topics such as movie, book, product, politic, and so on. Based on this condition, this research attempts to use the messages of twitter to review a movie by using opinion mining or sentiment analysis. Opinion mining refers to the application of natural language processing, computational linguistics, and text mining to identify or classify whether the movie is good or not based on message opinion. Support Vector Machine (SVM) is supervised learning methods that analyze data and recognize the patterns that are used for classification. This research concerns on binary classification which is classified into two classes. Those classes are positive and negative. The positive class shows good message opinion; otherwise the negative class shows the bad message opinion of certain movies. This justification is based on the accuracy level of \{SVM\} with the validation process uses 10-Fold cross validation and confusion matrix. The hybrid Partical Swarm Optimization (PSO) is used to improve the election of best parameter in order to solve the dual optimization problem. The result shows the improvement of accuracy level from 71.87% to 77%. }
}
</pre>

<a name="Mostafa20134241"></a><pre>
@article{<a href="twitmining.html#Mostafa20134241">Mostafa20134241</a>,
  title = {More than words: Social networks’ text mining for consumer brand sentiments },
  journal = {Expert Systems with Applications },
  volume = {40},
  number = {10},
  pages = {4241 - 4251},
  year = {2013},
  note = {},
  issn = {0957-4174},
  doi = {<a href="http://dx.doi.org/10.1016/j.eswa.2013.01.019">http://dx.doi.org/10.1016/j.eswa.2013.01.019</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0957417413000328">http://www.sciencedirect.com/science/article/pii/S0957417413000328</a>},
  author = {Mohamed M. Mostafa},
  keywords = {Consumer behavior},
  keywords = {Global brands},
  keywords = {Sentiment analysis},
  keywords = {Text mining},
  keywords = {Twitter },
  abstract = {Blogs and social networks have recently become a valuable resource for mining sentiments in fields as diverse as customer relationship management, public opinion tracking and text filtering. In fact knowledge obtained from social networks such as Twitter and Facebook has been shown to be extremely valuable to marketing research companies, public opinion organizations and other text mining entities. However, Web texts have been classified as noisy as they represent considerable problems both at the lexical and the syntactic levels. In this research we used a random sample of 3516 tweets to evaluate consumers’ sentiment towards well-known brands such as Nokia, T-Mobile, IBM, \{KLM\} and DHL. We used an expert-predefined lexicon including around 6800 seed adjectives with known orientation to conduct the analysis. Our results indicate a generally positive consumer sentiment towards several famous brands. By using both a qualitative and quantitative methodology to analyze brands’ tweets, this study adds breadth and depth to the debate over attitudes towards cosmopolitan brands. }
}
</pre>

<a name="PhridviRaj|Srinivas2014976"></a><pre>
@article{<a href="twitmining.html#PhridviRaj|Srinivas2014976">PhridviRaj|Srinivas2014976</a>,
  title = {Clustering Text Data Streams – A Tree based Approach with Ternary Function and Ternary Feature Vector },
  journal = {Procedia Computer Science },
  volume = {31},
  number = {},
  pages = {976 - 984},
  year = {2014},
  note = {2nd International Conference on Information Technology and Quantitative Management, \{ITQM\} 2014 },
  issn = {1877-0509},
  doi = {<a href="http://dx.doi.org/10.1016/j.procs.2014.05.350">http://dx.doi.org/10.1016/j.procs.2014.05.350</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S1877050914005274">http://www.sciencedirect.com/science/article/pii/S1877050914005274</a>},
  author = {PhridviRaj and Chintakindi Srinivas and C.V. GuruRao},
  keywords = {similarity},
  keywords = {ternary vector},
  keywords = {cluster},
  keywords = {data stream},
  keywords = {frequent item },
  abstract = {Abstract Data is the primary concern in data mining. Data Stream Mining is gaining a lot of practical significance with the huge online data generated from Sensors, Internet Relay Chats, Twitter, Facebook, Online Bank or \{ATM\} Transactions. The primary constraint in finding the frequent patterns in data streams is to perform only one time scan of the data with limited memory and requires less processing time. The concept of dynamically changing data is becoming a key challenge, what we call as data streams. In our present work, the algorithm is based on finding frequent patterns in the data streams using a tree based approach and to continuously cluster the text data streams being generated using a new ternary similarity measure defined. }
}
</pre>

<a name="PhridviRaj2014255"></a><pre>
@article{<a href="twitmining.html#PhridviRaj2014255">PhridviRaj2014255</a>,
  title = {Data Mining – Past, Present and Future – A Typical Survey on Data Streams },
  journal = {Procedia Technology },
  volume = {12},
  number = {},
  pages = {255 - 263},
  year = {2014},
  note = {The 7th International Conference Interdisciplinarity in Engineering, INTER-ENG 2013, 10-11 October 2013, Petru Maior University of Tirgu Mures, Romania },
  issn = {2212-0173},
  doi = {<a href="http://dx.doi.org/10.1016/j.protcy.2013.12.483">http://dx.doi.org/10.1016/j.protcy.2013.12.483</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S2212017313006683">http://www.sciencedirect.com/science/article/pii/S2212017313006683</a>},
  author = {M.S.B. PhridviRaj and C.V. GuruRao},
  keywords = {Clustering},
  keywords = {Streams},
  keywords = {Mining},
  keywords = {Dimensionality reduction},
  keywords = {Text stream},
  keywords = {Data streams },
  abstract = {Abstract Data Stream Mining is one of the area gaining lot of practical significance and is progressing at a brisk pace with new methods, methodologies and findings in various applications related to medicine, computer science, bioinformatics and stock market prediction, weather forecast, text, audio and video processing to name a few. Data happens to be the key concern in data mining. With the huge online data generated from several sensors, Internet Relay Chats, Twitter, Face book, Online Bank or \{ATM\} Transactions, the concept of dynamically changing data is becoming a key challenge, what we call as data streams. In this paper, we give the algorithm for finding frequent patterns from data streams with a case study and identify the research issues in handling data streams. }
}
</pre>

<a name="He2013464"></a><pre>
@article{<a href="twitmining.html#He2013464">He2013464</a>,
  title = {Social media competitive analysis and text mining: A case study in the pizza industry },
  journal = {International Journal of Information Management },
  volume = {33},
  number = {3},
  pages = {464 - 472},
  year = {2013},
  note = {},
  issn = {0268-4012},
  doi = {<a href="http://dx.doi.org/10.1016/j.ijinfomgt.2013.01.001">http://dx.doi.org/10.1016/j.ijinfomgt.2013.01.001</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0268401213000030">http://www.sciencedirect.com/science/article/pii/S0268401213000030</a>},
  author = {Wu He and Shenghua Zha and Ling Li},
  keywords = {Social media},
  keywords = {Facebook},
  keywords = {Twitter},
  keywords = {Case study},
  keywords = {Pizza industry},
  keywords = {Competitive analysis},
  keywords = {Competitive intelligence},
  keywords = {Competitor intelligence},
  keywords = {Actionable intelligence},
  keywords = {Text mining},
  keywords = {Content analysis },
  abstract = {Social media have been adopted by many businesses. More and more companies are using social media tools such as Facebook and Twitter to provide various services and interact with customers. As a result, a large amount of user-generated content is freely available on social media sites. To increase competitive advantage and effectively assess the competitive environment of businesses, companies need to monitor and analyze not only the customer-generated content on their own social media sites, but also the textual information on their competitors’ social media sites. In an effort to help companies understand how to perform a social media competitive analysis and transform social media data into knowledge for decision makers and e-marketers, this paper describes an in-depth case study which applies text mining to analyze unstructured text content on Facebook and Twitter sites of the three largest pizza chains: Pizza Hut, Domino's Pizza and Papa John's Pizza. The results reveal the value of social media competitive analysis and the power of text mining as an effective technique to extract business value from the vast amount of available social media data. Recommendations are also provided to help companies develop their social media competitive analysis strategy. }
}
</pre>

<a name="Miller201464"></a><pre>
@article{<a href="twitmining.html#Miller201464">Miller201464</a>,
  title = {Twitter spammer detection using data stream clustering },
  journal = {Information Sciences },
  volume = {260},
  number = {},
  pages = {64 - 73},
  year = {2014},
  note = {},
  issn = {0020-0255},
  doi = {<a href="http://dx.doi.org/10.1016/j.ins.2013.11.016">http://dx.doi.org/10.1016/j.ins.2013.11.016</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0020025513008037">http://www.sciencedirect.com/science/article/pii/S0020025513008037</a>},
  author = {Zachary Miller and Brian Dickinson and William Deitrick and Wei Hu and Alex Hai Wang},
  keywords = {Twitter},
  keywords = {Spam detection},
  keywords = {Clustering},
  keywords = {Data stream },
  abstract = {Abstract The rapid growth of Twitter has triggered a dramatic increase in spam volume and sophistication. The abuse of certain Twitter components such as “hashtags”, “mentions”, and shortened \{URLs\} enables spammers to operate efficiently. These same features, however, may be a key factor in identifying new spam accounts as shown in previous studies. Our study provides three novel contributions. Firstly, previous studies have approached spam detection as a classification problem, whereas we view it as an anomaly detection problem. Secondly, 95 one-gram features from tweet text were introduced alongside the user information analyzed in previous studies. Finally, to effectively handle the streaming nature of tweets, two stream clustering algorithms, StreamKM++ and DenStream, were modified to facilitate spam identification. Both algorithms clustered normal Twitter users, treating outliers as spammers. Each of these algorithms performed well individually, with StreamKM++ achieving 99% recall and a 6.4% false positive rate; and DenStream producing 99% recall and a 2.8% false positive rate. When used in conjunction, these algorithms reached 100% recall and a 2.2% false positive rate, meaning that our system was able to identify 100% of the spammers in our test while incorrectly detecting only 2.2% of normal users as spammers. }
}
</pre>

<a name="Yoon2013122"></a><pre>
@article{<a href="twitmining.html#Yoon2013122">Yoon2013122</a>,
  title = {A Practical Approach for Content Mining of Tweets },
  journal = {American Journal of Preventive Medicine },
  volume = {45},
  number = {1},
  pages = {122 - 129},
  year = {2013},
  note = {},
  issn = {0749-3797},
  doi = {<a href="http://dx.doi.org/10.1016/j.amepre.2013.02.025">http://dx.doi.org/10.1016/j.amepre.2013.02.025</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0749379713002432">http://www.sciencedirect.com/science/article/pii/S0749379713002432</a>},
  author = {Sunmoo Yoon and Noémie Elhadad and Suzanne Bakken},
  abstract = {Abstract Use of data generated through social media for health studies is gradually increasing. Twitter is a short-text message system developed 6 years ago, now with more than 100 million users generating over 300 million Tweets every day. Twitter may be used to gain real-world insights to promote healthy behaviors. The purposes of this paper are to describe a practical approach to analyzing Tweet contents and to illustrate an application of the approach to the topic of physical activity. The approach includes five steps: (1) selecting keywords to gather an initial set of Tweets to analyze; (2) importing data; (3) preparing data; (4) analyzing data (topic, sentiment, and ecologic context); and (5) interpreting data. The steps are implemented using tools that are publically available and free of charge and designed for use by researchers with limited programming skills. Content mining of Tweets can contribute to addressing challenges in health behavior research. }
}
</pre>

<a name="Lee20129623"></a><pre>
@article{<a href="twitmining.html#Lee20129623">Lee20129623</a>,
  title = {Mining spatio-temporal information on microblogging streams using a density-based online clustering method },
  journal = {Expert Systems with Applications },
  volume = {39},
  number = {10},
  pages = {9623 - 9641},
  year = {2012},
  note = {},
  issn = {0957-4174},
  doi = {<a href="http://dx.doi.org/10.1016/j.eswa.2012.02.136">http://dx.doi.org/10.1016/j.eswa.2012.02.136</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0957417412003995">http://www.sciencedirect.com/science/article/pii/S0957417412003995</a>},
  author = {Chung-Hong Lee},
  keywords = {Topic detection},
  keywords = {Text mining},
  keywords = {Microblogging},
  keywords = {Temporal analysis},
  keywords = {Spatial analysis },
  abstract = {Social networks have been regarded as a timely and cost-effective source of spatio-temporal information for many fields of application. However, while some research groups have successfully developed topic detection methods from the text streams for a while, and even some popular microblogging services such as Twitter did provide information of top trending topics for selection, it is still unable to fully support users for picking up all of the real-time event topics with a comprehensive spatio-temporal viewpoint to satisfy their information needs. This paper aims to investigate how microblogging social networks (i.e. Twitter) can be used as a reliable information source of emerging events by extracting their spatio-temporal features from the messages to enhance event awareness. In this work, we applied a density-based online clustering method for mining microblogging text streams, in order to obtain temporal and geospatial features of real-world events. By analyzing the events detected by our system, the temporal and spatial impacts of the emerging events can be estimated, for achieving the goals of situational awareness and risk management. }
}
</pre>

<a name="Lifna201586"></a><pre>
@article{<a href="twitmining.html#Lifna201586">Lifna201586</a>,
  title = {Identifying Concept-drift in Twitter Streams },
  journal = {Procedia Computer Science },
  volume = {45},
  number = {},
  pages = {86 - 94},
  year = {2015},
  note = {International Conference on Advanced Computing Technologies and Applications (ICACTA) },
  issn = {1877-0509},
  doi = {<a href="http://dx.doi.org/10.1016/j.procs.2015.03.093">http://dx.doi.org/10.1016/j.procs.2015.03.093</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S1877050915003294">http://www.sciencedirect.com/science/article/pii/S1877050915003294</a>},
  author = {C.S. Lifna and M. Vijayalakshmi},
  keywords = {Big Data},
  keywords = {Data Stream Mining ;Sliding Window},
  keywords = {Landmark Window},
  keywords = {Twitter},
  keywords = {Classification},
  keywords = {Topic ranking},
  keywords = {Concept-Drift. },
  abstract = {Abstract We live in a Big Data society, where the dignity of data is like exchange of currency. What we produce as data affords as access to different application, benefits, services, delivery etc… In today's world communication is mainly through social networking sites like, Twitter, Facebook, and Google+. Huge amount of data that is being generated and shared across these micro-blogging sites, serves as a good source of Big Data Streams for analysis. As the topic of discussion changes drastically, the relevance of data is temporal, which leads to concept-drift. Identification and handling of this concept-drift in such Big Data Streams is present area of interest. The state-of-the-art techniques for identifying trending topics in such data streams mainly concentrates on the frequency of the topic as the key parameter. Concentrating on such a weak indicator, reduces the precision of mining. This study puts forward a novel approach towards identifying concept-drift by initially grouping topics into classes and assigning weight-age for each class, using sliding window processing model upon Twitter streams. }
}
</pre>

<a name="Widener2014189"></a><pre>
@article{<a href="twitmining.html#Widener2014189">Widener2014189</a>,
  title = {Using geolocated Twitter data to monitor the prevalence of healthy and unhealthy food references across the \{US\} },
  journal = {Applied Geography },
  volume = {54},
  number = {},
  pages = {189 - 197},
  year = {2014},
  note = {},
  issn = {0143-6228},
  doi = {<a href="http://dx.doi.org/10.1016/j.apgeog.2014.07.017">http://dx.doi.org/10.1016/j.apgeog.2014.07.017</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0143622814001775">http://www.sciencedirect.com/science/article/pii/S0143622814001775</a>},
  author = {Michael J. Widener and Wenwen Li},
  keywords = {Food deserts},
  keywords = {Twitter},
  keywords = {Nutrition},
  keywords = {Spatial analysis},
  keywords = {Public health },
  abstract = {Abstract Mining the social media outlet Twitter for geolocated messages provides a rich database of information on people's thoughts and sentiments about myriad topics, like public health. Examining this spatial data has been particularly useful to researchers interested in monitoring and mapping disease outbreaks, like influenza. However, very little has been done to utilize this massive resource to examine other public health issues. This paper uses an advanced data-mining framework with a novel use of social media data retrieval and sentiment analysis to understand how geolocated tweets can be used to explore the prevalence of healthy and unhealthy food across the contiguous United States. Additionally, tweets are associated with spatial data provided by the \{US\} Department of Agriculture (USDA) of low-income, low-access census tracts (e.g. food deserts), to examine whether tweets about unhealthy foods are more common in these disadvantaged areas. Results show that these disadvantaged census tracts tend to have both a lower proportion of tweets about healthy foods with a positive sentiment, and a higher proportion of unhealthy tweets in general. These findings substantiate the methods used by the \{USDA\} to identify regions that are at risk of having low access to healthy foods. }
}
</pre>

<a name="Velardi2014153"></a><pre>
@article{<a href="twitmining.html#Velardi2014153">Velardi2014153</a>,
  title = {Twitter mining for fine-grained syndromic surveillance },
  journal = {Artificial Intelligence in Medicine },
  volume = {61},
  number = {3},
  pages = {153 - 163},
  year = {2014},
  note = {Text Mining and Information Analysis of Health Documents },
  issn = {0933-3657},
  doi = {<a href="http://dx.doi.org/10.1016/j.artmed.2014.01.002">http://dx.doi.org/10.1016/j.artmed.2014.01.002</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0933365714000049">http://www.sciencedirect.com/science/article/pii/S0933365714000049</a>},
  author = {Paola Velardi and Giovanni Stilo and Alberto E. Tozzi and Francesco Gesualdo},
  keywords = {Terminology clustering},
  keywords = {Twitter mining},
  keywords = {Micro-blog mining},
  keywords = {Patient's language learning},
  keywords = {Syndromic surveillance },
  abstract = {AbstractBackground Digital traces left on the Internet by web users, if properly aggregated and analyzed, can represent a huge information dataset able to inform syndromic surveillance systems in real time with data collected directly from individuals. Since people use everyday language rather than medical jargon (e.g. runny nose vs. respiratory distress), knowledge of patients’ terminology is essential for the mining of health related conversations on social networks. Objectives In this paper we present a methodology for early detection and analysis of epidemics based on mining Twitter messages. In order to reliably trace messages of patients that actually complain of a disease, first, we learn a model of naïve medical language, second, we adopt a symptom-driven, rather than disease-driven, keyword analysis. This approach represents a major innovation compared to previous published work in the field. Method We first developed an algorithm to automatically learn a variety of expressions that people use to describe their health conditions, thus improving our ability to detect health-related “concepts” expressed in non-medical terms and, in the end, producing a larger body of evidence. We then implemented a Twitter monitoring instrument to finely analyze the presence and combinations of symptoms in tweets. Results We first evaluate the algorithm's performance on an available dataset of diverse medical condition synonyms, then, we assess its utility in a case study of five common syndromes for surveillance purposes. We show that, by exploiting physicians’ knowledge on symptoms positively or negatively related to a given disease, as well as the correspondence between patients’ “naïve” terminology and medical jargon, not only can we analyze large volumes of Twitter messages related to that disease, but we can also mine micro-blogs with complex queries, performing fine-grained tweets classification (e.g. those reporting influenza-like illness (ILI) symptoms vs. common cold or allergy). Conclusions Our approach yields a very high level of correlation with flu trends derived from traditional surveillance systems. Compared with Google Flu, another popular tool based on query search volumes, our method is more flexible and less sensitive to changes in web search behaviors. }
}
</pre>

<a name="Micieli2012410"></a><pre>
@article{<a href="twitmining.html#Micieli2012410">Micieli2012410</a>,
  title = {Twitter as a tool for ophthalmologists },
  journal = {Canadian Journal of Ophthalmology / Journal Canadien d'Ophtalmologie },
  volume = {47},
  number = {5},
  pages = {410 - 413},
  year = {2012},
  note = {},
  issn = {0008-4182},
  doi = {<a href="http://dx.doi.org/10.1016/j.jcjo.2012.05.005">http://dx.doi.org/10.1016/j.jcjo.2012.05.005</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0008418212002943">http://www.sciencedirect.com/science/article/pii/S0008418212002943</a>},
  author = {Robert Micieli and Jonathan A. Micieli},
  abstract = {Twitter is a social media web site created in 2006 that allows users to post Tweets, which are text-based messages containing up to 140 characters. It has grown exponentially in popularity; now more than 340 million Tweets are sent daily, and there are more than 140 million users. Twitter has become an important tool in medicine in a variety of contexts, allowing medical journals to engage their audiences, conference attendees to interact with one another in real time, and physicians to have the opportunity to interact with politicians, organizations, and the media in a manner that can be freely observed. There are also tremendous research opportunities since Twitter contains a database of public opinion that can be mined by keywords and hashtags. This article serves as an introduction to Twitter and surveys the peer-reviewed literature concerning its various uses and original studies. Opportunities for use in ophthalmology are outlined, and a recommended list of ophthalmology feeds on Twitter is presented. Overall, Twitter is an underutilized resource in ophthalmology and has the potential to enhance professional collegiality, advocacy, and scientific research. }
}
</pre>

<a name="GalTzur2014115"></a><pre>
@article{<a href="twitmining.html#GalTzur2014115">GalTzur2014115</a>,
  title = {The potential of social media in delivering transport policy goals },
  journal = {Transport Policy },
  volume = {32},
  number = {},
  pages = {115 - 123},
  year = {2014},
  note = {},
  issn = {0967-070X},
  doi = {<a href="http://dx.doi.org/10.1016/j.tranpol.2014.01.007">http://dx.doi.org/10.1016/j.tranpol.2014.01.007</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0967070X14000225">http://www.sciencedirect.com/science/article/pii/S0967070X14000225</a>},
  author = {Ayelet Gal-Tzur and Susan M. Grant-Muller and Tsvi Kuflik and Einat Minkov and Silvio Nocera and Itay Shoor},
  keywords = {Social media},
  keywords = {Transport planning},
  keywords = {Transport policy},
  keywords = {Travel behaviour},
  keywords = {Text mining },
  abstract = {Abstract Information flow plays a central role in the development of transport policy, transport planning and the effective operation of the transport system. The recent upsurge in web enabled and pervasive technologies offer the opportunity of a new route for dynamic information flow that captures the views, needs and experiences of the travelling public in a timely and direct fashion through social media text posts. To date there is little published research, however, on how to realize this opportunity for the sector by capturing and analysing the text data. This paper provides an overview of the different categories of social media, the characteristics of its content and how these characteristics are reflected in transport-related posts. The research described in this paper includes a formulation of the goals for harvesting transport-related information from social media, the hypotheses to be tested to demonstrate that such information can provide valuable input to transport policy development or delivery and the challenges this involves. A hierarchical approach for categorizing transport-related information harvested from social media is presented. An explanatory study was designed, based on the understanding of the nature of social media content, the goals in harvesting it for transport planning and management purposes and existing text mining techniques. An exploratory case study is used to illustrate the process based on Twitter posts associated with particular \{UK\} sporting fixtures (i.e. football matches). The results demonstrate both the volume and pertinence of the information obtained. Whilst text-mining techniques have been applied in a number of other sectors (notably entertainment, business and the political arena), the use of information in the transport sector has some unique features that stem from both day-to-day operational practices and the longer term decision making processes surrounding the transport system – hence the significance and novelty of the results reported here. Many challenges in refining the methodology and techniques remain for future research, however the outcomes presented here are of relevance to a wide range of stakeholders in the transport and text mining fields. }
}
</pre>

<a name="Derczynski201532"></a><pre>
@article{<a href="twitmining.html#Derczynski201532">Derczynski201532</a>,
  title = {Analysis of named entity recognition and linking for tweets },
  journal = {Information Processing & Management },
  volume = {51},
  number = {2},
  pages = {32 - 49},
  year = {2015},
  note = {},
  issn = {0306-4573},
  doi = {<a href="http://dx.doi.org/10.1016/j.ipm.2014.10.006">http://dx.doi.org/10.1016/j.ipm.2014.10.006</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0306457314001034">http://www.sciencedirect.com/science/article/pii/S0306457314001034</a>},
  author = {Leon Derczynski and Diana Maynard and Giuseppe Rizzo and Marieke van Erp and Genevieve Gorrell and Raphaël Troncy and Johann Petrak and Kalina Bontcheva},
  keywords = {Information extraction},
  keywords = {Named entity recognition},
  keywords = {Entity disambiguation},
  keywords = {Microblogs},
  keywords = {Twitter },
  abstract = {Abstract Applying natural language processing for mining and intelligent information access to tweets (a form of microblog) is a challenging, emerging research area. Unlike carefully authored news text and other longer content, tweets pose a number of new challenges, due to their short, noisy, context-dependent, and dynamic nature. Information extraction from tweets is typically performed in a pipeline, comprising consecutive stages of language identification, tokenisation, part-of-speech tagging, named entity recognition and entity disambiguation (e.g. with respect to DBpedia). In this work, we describe a new Twitter entity disambiguation dataset, and conduct an empirical analysis of named entity recognition and disambiguation, investigating how robust a number of state-of-the-art systems are on such noisy texts, what the main sources of error are, and which problems should be further investigated to improve the state of the art. }
}
</pre>

<a name="Kontopoulos20134065"></a><pre>
@article{<a href="twitmining.html#Kontopoulos20134065">Kontopoulos20134065</a>,
  title = {Ontology-based sentiment analysis of twitter posts },
  journal = {Expert Systems with Applications },
  volume = {40},
  number = {10},
  pages = {4065 - 4074},
  year = {2013},
  note = {},
  issn = {0957-4174},
  doi = {<a href="http://dx.doi.org/10.1016/j.eswa.2013.01.001">http://dx.doi.org/10.1016/j.eswa.2013.01.001</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0957417413000043">http://www.sciencedirect.com/science/article/pii/S0957417413000043</a>},
  author = {Efstratios Kontopoulos and Christos Berberidis and Theologos Dergiades and Nick Bassiliades},
  keywords = {Micro-blogging},
  keywords = {Twitter},
  keywords = {Tweet},
  keywords = {Sentiment analysis},
  keywords = {Ontology },
  abstract = {The emergence of Web 2.0 has drastically altered the way users perceive the Internet, by improving information sharing, collaboration and interoperability. Micro-blogging is one of the most popular Web 2.0 applications and related services, like Twitter, have evolved into a practical means for sharing opinions on almost all aspects of everyday life. Consequently, micro-blogging web sites have since become rich data sources for opinion mining and sentiment analysis. Towards this direction, text-based sentiment classifiers often prove inefficient, since tweets typically do not consist of representative and syntactically consistent words, due to the imposed character limit. This paper proposes the deployment of original ontology-based techniques towards a more efficient sentiment analysis of Twitter posts. The novelty of the proposed approach is that posts are not simply characterized by a sentiment score, as is the case with machine learning-based classifiers, but instead receive a sentiment grade for each distinct notion in the post. Overall, our proposed architecture results in a more detailed analysis of post opinions regarding a specific topic. }
}
</pre>

<a name="BravoMarquez201486"></a><pre>
@article{<a href="twitmining.html#BravoMarquez201486">BravoMarquez201486</a>,
  title = {Meta-level sentiment models for big social data analysis },
  journal = {Knowledge-Based Systems },
  volume = {69},
  number = {},
  pages = {86 - 99},
  year = {2014},
  note = {},
  issn = {0950-7051},
  doi = {<a href="http://dx.doi.org/10.1016/j.knosys.2014.05.016">http://dx.doi.org/10.1016/j.knosys.2014.05.016</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0950705114002068">http://www.sciencedirect.com/science/article/pii/S0950705114002068</a>},
  author = {Felipe Bravo-Marquez and Marcelo Mendoza and Barbara Poblete},
  keywords = {Sentiment classification},
  keywords = {Twitter},
  keywords = {Meta-level features},
  keywords = {Opinion mining},
  keywords = {Social media },
  abstract = {Abstract People react to events, topics and entities by expressing their personal opinions and emotions. These reactions can correspond to a wide range of intensities, from very mild to strong. An adequate processing and understanding of these expressions has been the subject of research in several fields, such as business and politics. In this context, Twitter sentiment analysis, which is the task of automatically identifying and extracting subjective information from tweets, has received increasing attention from the Web mining community. Twitter provides an extremely valuable insight into human opinions, as well as new challenging Big Data problems. These problems include the processing of massive volumes of streaming data, as well as the automatic identification of human expressiveness within short text messages. In that area, several methods and lexical resources have been proposed in order to extract sentiment indicators from natural language texts at both syntactic and semantic levels. These approaches address different dimensions of opinions, such as subjectivity, polarity, intensity and emotion. This article is the first study of how these resources, which are focused on different sentiment scopes, complement each other. With this purpose we identify scenarios in which some of these resources are more useful than others. Furthermore, we propose a novel approach for sentiment classification based on meta-level features. This supervised approach boosts existing sentiment classification of subjectivity and polarity detection on Twitter. Our results show that the combination of meta-level features provides significant improvements in performance. However, we observe that there are important differences that rely on the type of lexical resource, the dataset used to build the model, and the learning strategy. Experimental results indicate that manually generated lexicons are focused on emotional words, being very useful for polarity prediction. On the other hand, lexicons generated with automatic methods include neutral words, introducing noise in the detection of subjectivity. Our findings indicate that polarity and subjectivity prediction are different dimensions of the same problem, but they need to be addressed using different subspace features. Lexicon-based approaches are recommendable for polarity, and stylistic part-of-speech based approaches are meaningful for subjectivity. With this research we offer a more global insight of the resource components for the complex task of classifying human emotion and opinion. }
}
</pre>

<a name="Abilhoa2014308"></a><pre>
@article{<a href="twitmining.html#Abilhoa2014308">Abilhoa2014308</a>,
  title = {A keyword extraction method from twitter messages represented as graphs },
  journal = {Applied Mathematics and Computation },
  volume = {240},
  number = {},
  pages = {308 - 325},
  year = {2014},
  note = {},
  issn = {0096-3003},
  doi = {<a href="http://dx.doi.org/10.1016/j.amc.2014.04.090">http://dx.doi.org/10.1016/j.amc.2014.04.090</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0096300314006304">http://www.sciencedirect.com/science/article/pii/S0096300314006304</a>},
  author = {Willyan D. Abilhoa and Leandro N. de Castro},
  keywords = {Knowledge discovery},
  keywords = {Text mining},
  keywords = {Keyword extraction},
  keywords = {Graph theory},
  keywords = {Centrality measures},
  keywords = {Twitter data },
  abstract = {Abstract Twitter is a microblog service that generates a huge amount of textual content daily. All this content needs to be explored by means of text mining, natural language processing, information retrieval, and other techniques. In this context, automatic keyword extraction is a task of great usefulness. A fundamental step in text mining techniques consists of building a model for text representation. The model known as vector space model, VSM, is the most well-known and used among these techniques. However, some difficulties and limitations of VSM, such as scalability and sparsity, motivate the proposal of alternative approaches. This paper proposes a keyword extraction method for tweet collections that represents texts as graphs and applies centrality measures for finding the relevant vertices (keywords). To assess the performance of the proposed approach, three different sets of experiments are performed. The first experiment applies \{TKG\} to a text from the Time magazine and compares its performance with that of the literature. The second set of experiments takes tweets from three different \{TV\} shows, applies \{TKG\} and compares it with \{TFIDF\} and KEA, having human classifications as benchmarks. Finally, these three algorithms are applied to tweets sets of increasing size and their computational running time is measured and compared. Altogether, these experiments provide a general overview of how \{TKG\} can be used in practice, its performance when compared with other standard approaches, and how it scales to larger data instances. The results show that \{TKG\} is a novel and robust proposal to extract keywords from texts, particularly from short messages, such as tweets. }
}
</pre>

<a name="Yan2013540"></a><pre>
@article{<a href="twitmining.html#Yan2013540">Yan2013540</a>,
  title = {Peri-Watchdog: Hunting for hidden botnets in the periphery of online social networks },
  journal = {Computer Networks },
  volume = {57},
  number = {2},
  pages = {540 - 555},
  year = {2013},
  note = {Botnet Activity: Analysis, Detection and Shutdown },
  issn = {1389-1286},
  doi = {<a href="http://dx.doi.org/10.1016/j.comnet.2012.07.016">http://dx.doi.org/10.1016/j.comnet.2012.07.016</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S1389128612002903">http://www.sciencedirect.com/science/article/pii/S1389128612002903</a>},
  author = {Guanhua Yan},
  keywords = {Twitter},
  keywords = {Botnet},
  keywords = {Anomaly detection},
  keywords = {Graph theory},
  keywords = {Centrality measure },
  abstract = {In order to evade detection of ever-improving defense techniques, modern botnet masters are constantly looking for new communication platforms for delivering C&amp;C (Command and Control) information. Attracting their attention is the emergence of online social networks such as Twitter, as the information dissemination mechanism provided by these networks can naturally be exploited for spreading botnet C&amp;C information, and the enormous amount of normal communications co-existing in these networks makes it a daunting task to tease out botnet C&amp;C messages. Against this backdrop, we explore graph-theoretic techniques that aid effective monitoring of potential botnet activities in large open online social networks. Our work is based on extensive analysis of a Twitter dataset that contains more than 40 million users and 1.4 billion following relationships, and mine patterns from the Twitter network structure that can be leveraged for improving efficiency of botnet monitoring. Our analysis reveals that the static Twitter topology contains a small-sized core sugraph, after removing which, the Twitter network breaks down into small connected components, each of which can be handily monitored for potential botnet activities. Based on this observation, we propose a method called Peri-Watchdog, which computes the core of a large online social network and derives the set of nodes that are likely to pass botnet C&amp;C information in the periphery of online social network. We analyze the time complexity of Peri-Watchdog under its normal operations. We further apply Peri-Watchdog on the Twitter graph injected with synthetic botnet structures and investigate the effectiveness of Peri-Watchdog in detecting potential C&amp;C information from these botnets. To verify whether patterns observed from the static Twitter graph are common to other online social networks, we analyze another online social network dataset, BrightKite, which contains evolution of social graphs formed by its users in half a year. We show not only that there exists a similarly relatively small core in the BrightKite network, but also this core remains stable over the course of BrightKite evolution. We also find that to accommodate the dynamic growth of BrightKite, the core has to be updated about every 18 days under a constrained monitoring capacity. }
}
</pre>

<a name="Lee201213338"></a><pre>
@article{<a href="twitmining.html#Lee201213338">Lee201213338</a>,
  title = {Unsupervised and supervised learning to evaluate event relatedness based on content mining from social-media streams },
  journal = {Expert Systems with Applications },
  volume = {39},
  number = {18},
  pages = {13338 - 13356},
  year = {2012},
  note = {},
  issn = {0957-4174},
  doi = {<a href="http://dx.doi.org/10.1016/j.eswa.2012.05.068">http://dx.doi.org/10.1016/j.eswa.2012.05.068</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0957417412007841">http://www.sciencedirect.com/science/article/pii/S0957417412007841</a>},
  author = {Chung-Hong Lee},
  keywords = {Stream mining},
  keywords = {Data mining},
  keywords = {Event evaluation},
  keywords = {Social networks },
  abstract = {Due to the explosive growth of social-media applications, enhancing event-awareness by social mining has become extremely important. The contents of microblogs preserve valuable information associated with past disastrous events and stories. To learn the experiences from past events for tackling emerging real-world events, in this work we utilize the social-media messages to characterize real-world events through mining their contents and extracting essential features for relatedness analysis. On one hand, we established an online clustering approach on Twitter microblogs for detecting emerging events, and meanwhile we performed event relatedness evaluation using an unsupervised clustering approach. On the other hand, we developed a supervised learning model to create extensible measure metrics for offline evaluation of event relatedness. By means of supervised learning, our developed measure metrics are able to compute relatedness of various historical events, allowing the event impacts on specified domains to be quantitatively measured for event comparison. By combining the strengths of both methods, the experimental results showed that the combined framework in our system is sensible for discovering more unknown knowledge about event impacts and enhancing event awareness. }
}
</pre>

<a name="Mansmann2014120"></a><pre>
@article{<a href="twitmining.html#Mansmann2014120">Mansmann2014120</a>,
  title = {Discovering \{OLAP\} dimensions in semi-structured data },
  journal = {Information Systems },
  volume = {44},
  number = {},
  pages = {120 - 133},
  year = {2014},
  note = {},
  issn = {0306-4379},
  doi = {<a href="http://dx.doi.org/10.1016/j.is.2013.09.002">http://dx.doi.org/10.1016/j.is.2013.09.002</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0306437913001270">http://www.sciencedirect.com/science/article/pii/S0306437913001270</a>},
  author = {Svetlana Mansmann and Nafees Ur Rehman and Andreas Weiler and Marc H. Scholl},
  keywords = {Data warehousing},
  keywords = {OLAP},
  keywords = {Multidimensional data model},
  keywords = {Semi-structured data },
  abstract = {Abstract \{OLAP\} cubes enable aggregation-centric analysis of transactional data by shaping data records into measurable facts with dimensional characteristics. A multidimensional view is obtained from the available data fields and explicit relationships between them. This classical modeling approach is not feasible for scenarios dealing with semi-structured or poorly structured data. We propose to the data warehouse design methodology with a content-driven discovery of measures and dimensions in the original dataset. Our approach is based on introducing a data enrichment layer responsible for detecting new structural elements in the data using data mining and other techniques. Discovered elements can be of type measure, dimension, or hierarchy level and may represent static or even dynamic properties of the data. This paper focuses on the challenge of generating, maintaining, and querying discovered elements in \{OLAP\} cubes. We demonstrate the power of our approach by providing \{OLAP\} to the public stream of user-generated content on the Twitter platform. We have been able to enrich the original set with dynamic characteristics, such as user activity, popularity, messaging behavior, as well as to classify messages by topic, impact, origin, method of generation, etc. Knowledge discovery techniques coupled with human expertise enable structural enrichment of the original data beyond the scope of the existing methods for obtaining multidimensional models from relational or semi-structured data. }
}
</pre>

<a name="Hassard2013566"></a><pre>
@article{<a href="twitmining.html#Hassard2013566">Hassard2013566</a>,
  title = {Assessing the Impact of the Fukushima Nuclear Disaster on Policy Dynamics and the Public Sphere },
  journal = {Procedia Environmental Sciences },
  volume = {17},
  number = {},
  pages = {566 - 575},
  year = {2013},
  note = {The 3rd International Conference on Sustainable Future for Human Security, \{SUSTAIN\} 2012, 3-5 November 2012, Clock Tower Centennial Hall, Kyoto University, \{JAPAN\} },
  issn = {1878-0296},
  doi = {<a href="http://dx.doi.org/10.1016/j.proenv.2013.02.072">http://dx.doi.org/10.1016/j.proenv.2013.02.072</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S1878029613000741">http://www.sciencedirect.com/science/article/pii/S1878029613000741</a>},
  author = {Harry A. Hassard and Joshua K.Y. Swee and Moustafa Ghanem and Hironobu Unesaki},
  keywords = {Fukushima},
  keywords = {Twitter},
  keywords = {Social media},
  keywords = {Nuclear profile},
  keywords = {Public sphere},
  keywords = {Policy dynamics},
  keywords = {AHP},
  keywords = {Headlines },
  abstract = {Social and political fallout following the March 2011 Fukushima-Daiichi nuclear disaster permanently altered the zeitgeist of global public attitude towards nuclear power and towards energy technology in general. This area of public policy, which in Japan is particularly opaque and stagnant, was forced into a period of energy sector review amid domestic and worldwide debate. This study explores novel methodologies for measuring these developments, covering the 1) framing effects of traditional media and the 2) user-sourced content of social media. This quantitative approach yielded the following hypothesis verifications; 1) in an AHP-style online survey, exposure to real and simulated nuclear-related disaster headlines reduced collective partiality towards nuclear power by 3% and 4% respectively, and 2) retrospective opinion mining of Twitter procured an relative increase in negative nuclear-related posts of 38% and 134% in Japanese and English respectively, from the pre to post-Fukushima world. Using nuclear power and Fukushima as a case study, this paper attempts to elucidate both the influence of media on the public sphere, and the influence of the public sphere on policy and policymakers. From the results it is possible to make the conjecture that a lack of scientific education with regard to energy issues increases the former influence, and similarly reduces the latter. }
}
</pre>

<a name="Kavanaugh2012480"></a><pre>
@article{<a href="twitmining.html#Kavanaugh2012480">Kavanaugh2012480</a>,
  title = {Social media use by government: From the routine to the critical },
  journal = {Government Information Quarterly },
  volume = {29},
  number = {4},
  pages = {480 - 491},
  year = {2012},
  note = {Social Media in Government - Selections from the 12th Annual International Conference on Digital Government Research (dg.o2011) },
  issn = {0740-624X},
  doi = {<a href="http://dx.doi.org/10.1016/j.giq.2012.06.002">http://dx.doi.org/10.1016/j.giq.2012.06.002</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0740624X12000871">http://www.sciencedirect.com/science/article/pii/S0740624X12000871</a>},
  author = {Andrea L. Kavanaugh and Edward A. Fox and Steven D. Sheetz and Seungwon Yang and Lin Tzy Li and Donald J. Shoemaker and Apostol Natsev and Lexing Xie},
  keywords = {Digital government},
  keywords = {Social media},
  keywords = {Crisis informatics},
  keywords = {Information visualization},
  keywords = {Word cloud},
  keywords = {Focus group study},
  keywords = {Civic organizations },
  abstract = {Social media and online services with user-generated content (e.g., Twitter, Facebook, Flickr, YouTube) have made a staggering amount of information (and misinformation) available. Government officials seek to leverage these resources to improve services and communication with citizens. Significant potential exists to identify issues in real time, so emergency managers can monitor and respond to issues concerning public safety. Yet, the sheer volume of social data streams generates substantial noise that must be filtered in order to detect meaningful patterns and trends. Important events can then be identified as spikes in activity, while event meaning and consequences can be deciphered by tracking changes in content and public sentiment. This paper presents findings from a exploratory study we conducted between June and December 2010 with government officials in Arlington, \{VA\} (and the greater National Capitol Region around Washington, D.C.), with the broad goal of understanding social media use by government officials as well as community organizations, businesses, and the public at large. A key objective was also to understand social media use specifically for managing crisis situations from the routine (e.g., traffic, weather crises) to the critical (e.g., earthquakes, floods). }
}
</pre>

<a name="Clark20112"></a><pre>
@article{<a href="twitmining.html#Clark20112">Clark20112</a>,
  title = {Text Normalization in Social Media: Progress, Problems and Applications for a Pre-Processing System of Casual English },
  journal = {Procedia - Social and Behavioral Sciences },
  volume = {27},
  number = {},
  pages = {2 - 11},
  year = {2011},
  note = {Computational Linguistics and Related Fields },
  issn = {1877-0428},
  doi = {<a href="http://dx.doi.org/10.1016/j.sbspro.2011.10.577">http://dx.doi.org/10.1016/j.sbspro.2011.10.577</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S1877042811024049">http://www.sciencedirect.com/science/article/pii/S1877042811024049</a>},
  author = {Eleanor Clark and Kenji Araki},
  keywords = {Natural Language Processing},
  keywords = {Machine Translation},
  keywords = {Social Media},
  keywords = {Twitter},
  keywords = {Text Normalization },
  abstract = {Abstract The rapid expansion in user-generated content on the Web of the 2000s, characterized by social media, has led to Web content featuring somewhat less standardized language than the Web of the 1990s. User creativity and individuality of language creates problems on two levels. The first is that social media text is often unsuitable as data for Natural Language Processing tasks such as Machine Translation, Information Retrieval and Opinion Mining, due to the irregularity of the language featured. The second is that non-native speakers of English, older Internet users and non-members of the “in-group” often find such texts difficult to understand. This paper discusses problems involved in automatically normalizing social media English, various applications for its use, and our progress thus far in a rule-based approach to the issue. Particularly, we evaluate the performance of two leading open source spell checkers on data taken from the microblogging service Twitter, and measure the extent to which their accuracy is improved by pre-processing with our system. We also present our database rules and classification system, results of evaluation experiments, and plans for expansion of the project. }
}
</pre>

<a name="Price2013569"></a><pre>
@article{<a href="twitmining.html#Price2013569">Price2013569</a>,
  title = {SubSift web services and workflows for profiling and comparing scientists and their published works },
  journal = {Future Generation Computer Systems },
  volume = {29},
  number = {2},
  pages = {569 - 581},
  year = {2013},
  note = {Special section: Recent advances in e-Science },
  issn = {0167-739X},
  doi = {<a href="http://dx.doi.org/10.1016/j.future.2011.10.016">http://dx.doi.org/10.1016/j.future.2011.10.016</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0167739X11002238">http://www.sciencedirect.com/science/article/pii/S0167739X11002238</a>},
  author = {Simon Price and Peter A. Flach and Sebastian Spiegler and Christopher Bailey and Nikki Rogers},
  keywords = {Workflows},
  keywords = {Web services},
  keywords = {Ranking },
  abstract = {Scientific researchers, laboratories, organisations and research communities can be profiled and compared by analysing their published works, including documents ranging from academic papers to web sites, blog posts and Twitter feeds. This paper describes how the vector space model from information retrieval, more normally associated with full text search, has been employed in the open source SubSift software to support workflows to profile and compare such collections of documents. SubSift was originally designed to match submitted conference or journal papers to potential peer reviewers based on the similarity between the paper’s abstract and the reviewer’s publications as found in online bibliographic databases such as Google Scholar. The software is implemented as a family of \{RESTful\} web services that, composed into a re-useable workflow, have already been used to support several major data mining conferences. Alternative workflows and service compositions are now enabling other interesting applications, such as expert finding for the press and media, organisational profiling, and suggesting potential interdisciplinary research partners. This work is a useful generalisation and proof-of-concept realisation of an engineering solution to enable \{RESTful\} services to be assembled in workflows to analyse general content in a way that is not immediately available elsewhere. The challenges and lessons learned in the implementation and use of SubSift are discussed. }
}
</pre>

<a name="ServiaRodríguez20142582"></a><pre>
@article{<a href="twitmining.html#ServiaRodríguez20142582">ServiaRodríguez20142582</a>,
  title = {A tie strength based model to socially-enhance applications and its enabling implementation: mySocialSphere },
  journal = {Expert Systems with Applications },
  volume = {41},
  number = {5},
  pages = {2582 - 2594},
  year = {2014},
  note = {},
  issn = {0957-4174},
  doi = {<a href="http://dx.doi.org/10.1016/j.eswa.2013.10.006">http://dx.doi.org/10.1016/j.eswa.2013.10.006</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0957417413008117">http://www.sciencedirect.com/science/article/pii/S0957417413008117</a>},
  author = {Sandra Servia-Rodríguez and Rebeca P. Díaz-Redondo and Ana Fernández-Vilas and Yolanda Blanco-Fernández and José J. Pazos-Arias},
  keywords = {Social Web},
  keywords = {Software as a Service},
  keywords = {Data mining},
  keywords = {Tie strength},
  keywords = {Social spheres },
  abstract = {Abstract The growing omnipresence of the Social Web and the increasingly number of services in the Cloud have created a favourable atmosphere to develop socially-enhanced services, that is, services which are aware of the social dimension of the users to improve their experience in the Cloud. This paper introduces a model and an architecture for socially-enhanced services by mining interaction information across different Social Web sites. Most of the existing social applications require knowing who are the users socially-linked to each individual by exploring topological connections in social networks or, even, calculating the interactions network that underlies social sites. However these approaches are, on the one hand, hardly scalable when the number of users grows in the interaction network and, on the other hand, tightly coupled to the social application and so hardly reusable. The key contribution of this paper is a user-centred model whose goal is not to infer the aforementioned interaction network, but to build users’ social spheres. That is, assessing the strength and the context of the user’s ties by using signs of interaction available from social sites \{APIs\} (private messages, retweets, mentions, …) with user’s permission. To this aim, contrary to previous approaches, we take into account (i) different interaction types and contexts, (ii) the time in which interactions occur, (iii) the people involved in them and (iv) the interactions rhythms with the rest of user’s contacts. A prototype of this service has been implemented in order to, not only validate the tie strength model, but also to deploy some pilot experiences. }
}
</pre>

<a name="Scanfeld2010182"></a><pre>
@article{<a href="twitmining.html#Scanfeld2010182">Scanfeld2010182</a>,
  title = {Dissemination of health information through social networks: Twitter and antibiotics },
  journal = {American Journal of Infection Control },
  volume = {38},
  number = {3},
  pages = {182 - 188},
  year = {2010},
  note = {},
  issn = {0196-6553},
  doi = {<a href="http://dx.doi.org/10.1016/j.ajic.2009.11.004">http://dx.doi.org/10.1016/j.ajic.2009.11.004</a>},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0196655310000349">http://www.sciencedirect.com/science/article/pii/S0196655310000349</a>},
  author = {Daniel Scanfeld and Vanessa Scanfeld and Elaine L. Larson},
  keywords = {Antibiotic},
  keywords = {resistance},
  keywords = {Web 2.0},
  keywords = {Twitter },
  abstract = {Background This study reviewed Twitter status updates mentioning “antibiotic(s)” to determine overarching categories and explore evidence of misunderstanding or misuse of antibiotics. Methods One thousand Twitter status updates mentioning antibiotic(s) were randomly selected for content analysis and categorization. To explore cases of potential misunderstanding or misuse, these status updates were mined for co-occurrence of the following terms: “cold + antibiotic(s),” “extra + antibiotic(s),” “flu + antibiotic(s),” “leftover + antibiotic(s),” and “share + antibiotic(s)” and reviewed to confirm evidence of misuse or misunderstanding. Results Of the 1000 status updates, 971 were categorized into 11 groups: general use (n = 289), advice/information (n = 157), side effects/negative reactions (n = 113), diagnosis (n = 102), resistance (n = 92), misunderstanding and/or misuse (n = 55), positive reactions (n = 48), animals (n = 46), other (n = 42), wanting/needing (n = 19), and cost (n = 8). Cases of misunderstanding or abuse were identified for the following combinations: “flu + antibiotic(s)” (n = 345), “cold + antibiotic(s)” (n = 302), “leftover + antibiotic(s)” (n = 23), “share + antibiotic(s)” (n = 10), and “extra + antibiotic(s)” (n = 7). Conclusion Social media sites offer means of health information sharing. Further study is warranted to explore how such networks may provide a venue to identify misuse or misunderstanding of antibiotics, promote positive behavior change, disseminate valid information, and explore how such tools can be used to gather real-time health data. }
}
</pre>

<hr><p><em>This file was generated by
<a href="http://www.lri.fr/~filliatr/bibtex2html/">bibtex2html</a> 1.96.</em></p>
</body>
</html>
